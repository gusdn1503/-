끝까지 작성한 파일이 날라가 간략하게만 다시 적겠습니다....ㅠ

softmax classification은 binary classification 처럼 한 개의 상태이냐 아니냐를 판단하는 것이 아닌
여러개의 상태 중 어떤 것에 속하는지 분류할때 사용됩니다. 
이떄 식은 h(x)=xW+b의 기본 가설함수를 따르지만 
input x값은 n*input으로 주어지는 특성들의 개수 y와 h(x)는 n*구별해야할 라벨 개수
따라서 W는 특성개수 *라벨 개수가 되고 이 행렬의 각 열은 가설행렬의 각 라벨을 결정하게 됩니다.

이번에는 sigmoid가 아닌 softmax function을 사용하여 기본 logits 벡터들을 모두 0~1사이이고 합하여 1이 되는
벡터로 바꾸도록 합니다. 이는 각 예측값을 합쳤을때 1이되는 확률 값으로 변환해주기 위함입니다.  
softmax= e^yi/sigma(e^yj) 
결국 우리 가설함수 H(x)=softmax(h(x))가 됩니다.

이제 cost 함수를 알아볼텐데 이번에 알아볼 cost 함수는 cross-entropy cost funciton으로
D(H(x),y)=sigma(yi*(-log(H(x)))) 입니다.
이때 i는 라벨의 개수만큼입니다.
이해해보자면 라벨이 1으로 표시된 부분 즉, 실제 그 트레이닝 데이터의 정답인 부분의 예측함수가 1로부터 
멀리 떨어진 만큼을 cost로 설정하는 것이고 이는 앞의 logistic cost와 결국 같은 메커니즘입니다.
마지막으로 이를 전체 트레이닝 데이터의 개수만큼 모두 더하고 나누어 평균을 내면 
우리가 원하는 최종 Loss=1/n*sigam(D(H(x),y))가 완성됩니다.

[tensorflow code]
#기존 logistic 부분과 다른 부분 위주로 설명하겠습니다. 
#주의할 부분은 이제 y의 데이터 형식이 n*label이므로 
y=tf.placeholder("float",[None,3])이런식으로 써주어야하고
W=tf.Variable(tf.random_normal([input,label]),name='weight')로 적어야합니다.
#이때 가설은 최종적으로 softmax한 것을 가설로 정하므로
hypo=tf.nn.softmax(tf.matmul(x,W)+b)
cost=tf.reduce_mean(-tf.reduce_sum(y*tf.log(hypo),axis=1))
#이부분이 조금 이해하기 까다로울 수 있는데 axis=1은 행 안에서 합치라는 뜻입니다. 
#따라서 실제 y*H(x)의 모양이 [ [0 0.8 0], [0 0 0.7] ] 처럼 나오고 (elementwise곱이기 떄문)
#이를 [0.8, 0.7] 이런식으로 만들어줍니다
#즉 우리의 cross entropy cost function에서 라벨이 아닌 부분은 0이 곱해져 사라졋던 것을 행해주는 부분입니다.
#그리고 이를 axis없이 트레이닝 데이터 개수로 평균내면 마지막 스칼라로 된 loss를 얻을 수 있습니다.

#Fancy sofrmax
#조금 더 세세한 부분을 다루어 보겠습니다.
a=sess.run(hypo,feed_dict=feed)
print(a,sess.run(tf.argmax(a,1)))
#이는 행속에서 가장 높은 애의 index를 뱉어내줍니다. 
cost=tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,labels=y)
#이 최신 버젼을 이용하면 tf.matmul(x,W)+b를 만들어 y와 넣으면 바로 스칼라 cost를 구해줍니다.
#단 이때 y는 역시 one-hot encodig된 데이터여야겠죠! 
#만일 그러지 않은 경우 0~6까지 정수형을 되있을 경우에 one-hot화 하는 방법을 알아봅시다.
y_before=tf.placeholder(tf.int32,[None,1]) # 0~6의 자료형이 주어졌을때
y=tf.one_hot(y,label=7)
y=tf.reshape(y,[-1,label=7])
#그냥 온핫하면 [ [[100]] [[011] ] 처럼 차원을 하나 더 주기 때문에 reshape을 이용하여 n*7의 형태로 바꾸어줍시다.

#나머지는 모두 같고 예측치와 정확도를 한번 구해봅시다.
prediction=tf.argmax(hypo,1) #가장 높은 확률이 나온 애로 predict하자고 결정
correct_prediction=tf.equal(prediction,tf.argmax(y,1))#실제 라벨값과 예측 값이 같으면 1 아니면 0의 행렬화
accuracy=tf.reduce_mean(tf.cast(correct_prediction,tf.float32))#다시 float으로 바꾸고 평균을 구한다.



