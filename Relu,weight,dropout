Neural network를 위한 간과하기 쉽지만 매우 중요한 몇 가지를 알아보자..!


먼저 우리가 그동안 sigmoid 함수를 썻었던 것을 기억해보자. 
이는 0~1사이 값을 가지게 하기 위함인데 0 양 옆으로 조금만 멀리 떨어져도 대부분의 값이
0에 매우 가까운 0.01 0.0001 등이 되거나 거의 1에 가까워지게 된다. 
그런데 이 것이 deep하지 않은 즉 layer가 몇 개 없는 모델에서는 큰 문제가 되지 않았다. 
하지만 layer를 많이 쌓아 deep nn이 될수록 문제가 발생하게 된다. 
조금 예를 들어 설명해보자. 
x y의 곱을 나타내는 x*y가 있고 x y각각의 노드가 그에 연결되어 있다고 해보자. 
이때 xy를 각각 x,y로 편미분 할시 y,x가 나온다. 즉, x y가 x*y라는 '다음' 노드에 
미치는 영향의 변동폭이 각각 y,x가 된다는 것이다. 그런데 이 x,y 역시 다른 노드들에 의해서
생성된 노드라고 하면 y앞에 sigmoid 함수가 붙어 있을 것이고 그에 따라 위에 언급한 이유에 의해
0에 가까운 0.01,0.0001 등이 될 것이다. 즉 x의 변동폭이 0.0001등으로 매우 작게 잡힌다는 것이다. 
게다가 layer가 깊어질 수록 이런 현상이 중첩해서 이루어지기 떄문에 앞부분 layer는 사실상 결과에
거의 영향을 미칠 수 없게 되어 backpropagation에 문제가 발생한다.
따라서 이 문제 때문에 최종 layer와 멀수록 gradient가 사라지게 되고 그만큼 학습하기 어려워지는
문제(vanishing gradient)가 발생하여 1986-2006의 20년 간의 암흑기를 초래했다.
이 문제를 해결하기 위해 유명한 Hinton 교수가 지적한 그동안의 문제점을 보면
We used the wrong type of non-linearity-->sigmoid가 잘못되었다.
Rectified Linear Unit(ReLU)라고 0이하는 없애고 0이상은 선형을 취하는 것이다. 
tf.nn.relu(tf.matmul(x,W)+b)으로 간단히 쓸 수 있고 
마지막 단에만 sigmoid나 softmax를 쓴다. 
RelU외에도 activation function에는 여러가지가 있다. 
tanh,Leaky ReLU, Maxout, ElU 등이 있는데 각각의 장 단점에 대해 추후 자세히 포스팅 하겠다. 

Hinton 교수가 문제로 지적한 또 다른 문제는 
We initialized the weights in a stupid way 
즉 초기 값을 너무 대충 랜덤하게 지었다는 것이다. 
초기 값은 cost를 줄이는 속도와 local minimum에 빠지지 않게 하기 위해
매우 중요한 변수였던 것이다. 
극단적으로 초기값을 0으로 주면 그에 따른 그 전의 weight이 모두 학습이 되지 않기도 하기 때문이다.
Hinton은 2006년 A Fast Learning Algorithm for Deep Belief Nets(DBM)를 발표했고
Restricted Boatman Machine(RBM)이란 것을 사용해서 초기화 한것이 DBM이다. 
그 동작 방식을 간단히 살펴보자.
layer들을 딱 두개 씩만 봐보자. 
첫 번째 층에서 forward 해서 두번 째 층으로 가게 하고 거꾸로 두 번 째 층에서 weight을 이용하여 
다시 첫번째 층으로 오게해 그 값들이 처음과 같게 만드는 것이 목표다. (각각 encode,decode라고 한다.)
이렇게 pre training을 하면 상당한 성능 증가를 볼 수 있다. 
Xavier/He initialization이 이후에 등장하였는데 상당히 간단한 방법으로 큰 효고를 볼 수 있다.
W=np.random.randn(input_num,output_num)/np.sqrt(input_num(/2*He))
W=tf.get_variable("W1",shape=[784,256],initializer=tf.contrib.layers.xavier_initializer())
처럼 쓰면 된다. 이 코드를 실행해보면 cost가 처음부터 매우 작은 모습을 볼 수 있고 
상당한 정확도 향상을 볼 수 있다. 
이 분야는 지금도 매우 많은 연구가 이루어지고 있는 과정이기에 최신 동향에 주의해야한다. 


우리가 가지고 있는 또 다른 문제는 너무 training data에 overfitting되어 trian은 잘 푸는데
test를 잘 풀지 못하는 것이다. 
깊게 nn을 할 수록 많은 변수가 사용되고 hyperplane이 그만큼 구불구불하게 되어 
overfitting이 발생할 확률이 커지게 된다. 
따라서 regularization의 중요성이 대두되고 현재는 주로
l2reg=lambda*tf.reduce_sum(tf.square(w))를 쓴다. 
nn에서는 overfitting을 방지하기 위한 방법이 하나 더 있는데 바로 DROP OUT이다. 
random하게 몇 몇 뉴런의 학습을 중단시키자는 것이다. 
drop out을 구현할때는 keep_prob=tf.palceholder("float")으로 하나의 변수를 만들고
L1=tf.nn.relu(tf.matmul(x,W1)+b1)
L1=tf.nn.dropout(L1,keep_prob) 
이 처럼 하나의 단을 더 만든다. 보통 0.5~0.7로 사용한다. 주의해야할 점은 
sess.run(optimizer,feed_dict{X:--,Y:--,keep_prob:0.7}) 이렇게 학습할 때는 끄도록 만들고
accuracy.eval({x:--,y:--,keep_prob:1})로 test단계에서는 모두 살려서 테스트 해야한다는 것이다. 

또 알아두면 좋을 것은 optimizer들인데 이 optimizer에 따라 초기 값에서 cost를 줄이는 속도가 
매우 큰 차이가 나고, 기존 sgd는 아예 초기에서 못 벗어나는 한계 등도 있기 때문에 
경사에 '관성'을 도입한 여러 방법들이 나왔고 Adam optimizer가 현재 널리 쓰이고 있다. 
수리적으로 그 원리를 알아보기 위해서는
http://ruder.io/optimizing-gradient-descent/
http://shuuki4.github.io/deep%20learning/2016/05/20/Gradient-Descent-Algorithm-Overview.html
를 참고하도록 하자!
Ensemble을 알아보자. 
주어진 Data set에서 똑같은 형태의 nn을 여러 개 만든 다음에 각각 다른 초기값을 주어
학습을 시키고 나중에 합쳐서 argmax하여 가장 확률 값이 크게 나온 것을 정답으로 사용한다. 
이 경우 최대 정확도가 2~4%까지 상승하기 떄문에 꼭 해주면 좋은 작업이다. 

우리가 마지막으로 알면 좋은 것은 
이미지넷 오답률을 3% 대로 줄인 ResNet에서 He가 쓴 방법처럼 fast forward방법
split&merge(cnn) /옆으로 전달하는 Recurrent Network 등이 실제로 좋은 성능을 낼 수 있는 것을
통해 알 수 있듯이 아주 다양한 방법으로 nn을 만들 수 있고 only limit은 우리의 상상력일 뿐이다...!
