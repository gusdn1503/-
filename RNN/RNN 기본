RNN은 Recurrent Neural Network의 약자로 
Sequence data를 처리하기에 최적화된 NN이라 할 수 있다.
예를 들어 우리가 
hi, hello라는 단어를 학습시킨다고 할 때
같은 h 뒤지만 i, e 두개가 모두 올 수 있기 때문에 
이를 학습시키려면 전 단어들의 input을 활용하는 방법이 필요하고 그래서 나온 것이 RNN인 것이다. 
RNN은 굉장히 활용방법이 많은데
(인풋,아웃풋) 개수가 이러한 flexibility를 주는 원천이다.
다양한 예제들을 살펴보면
Language Modeling,sentiment classification(다수,1)speech recognition,
Machine Translation(다수,다수), Conversation Machine,vidieo classification(다수,다수 단 일대일 대응)
Image Captioning(1,다수) 등 그 방법이 아주 다양하다.

즉 과거 state h(t-1)과 input x(t)의 두가지 인풋을 가지고 새로운 상태 h(t)를 도출하는 방법이다.
간단한 식을 써보면
Ht=tanh(WhhHt-1 + WxhXt)
Yt=WhyHt이다. 
그런데 여기서 중요한 점은 왜 tanh를 쓰는지 아는 등의 딥러닝의 기저에 깔려있는 이유를 고민해 보는 것이다.
tanh는 하이퍼볼릭탄젠트로 비선형 함수란 것이 큰 특징이다. 
선형 함수로 몇 겹의 네트워크를 쌓든 y(x)=c*c*c*x=a*x 로 간단히 나타낼 수 있다. 
즉 히든 레이어가 없는 네트워크로도 표현할 수 있다는 것이고 층을 쌓는 혜택을 얻고 싶으면 따라서
활성함수로 반드시 비선형 함수를 사용해야하는 것이다..! (출처:밑바닥부터 시작하는 딥러닝)

[실습]
#RNN을 쓸때는 먼저 text를 unique char로 바꾸고 이를 one hot encoding해줘야하는 점을 기억하자
#RNN의 input,output shape는(batch_size,sequence_length,hidden_size)와 같은 식으로 쓴다.
#batch_size:단어의 개수처럼 한번에 줄 데이터의 개수
#sequence_length:단어의 길이 처럼 알파벳 몇개를 받을 것인가를 결정
#hidden_size:출력의 크기 (1,1,NONE)을 넣으면 (1,1,2)를 출력해줌

X_data=np.array([[h,e,l,l,lo]],dtype=np.float32) #이때 h=[1,0,0,0]처럼 one-hot encoding하는 식으로 sequence 데이터 표시
x=tf.placeholder(tf.float32,[None,sequence_length,input_dim]) #알파벳수와 구별해야하는 unique char수
y=tf.placeholder(tf.int32,[None,sequence_length])
cell=tf.contrib.rnn.BasicRNNCell(or BasicLSTMCell)(num_units=hidden_size)
initial_state=cell.zero_stat(batch_sie,tf.float32)
outputs,_states=tf.nn.dynamic_rnn(cell,x,initial_state=initial_stae,dtype=tf.float32)
#predictions=tf.constant([[[0.3,0.7],[,],[,]]],dtype=float32)
weigths=tf.ones([batch_size,sequence_length])

seqeunce_loss=tf.contrib.seq2seq.sequence_loss(logits=prediction,targets=y_data,weights=weights)
loss=tf.reduce_mean(sequence_loss) #rnn에 나오는 것을 바로 logit에 넣으면 사실 안좋다.
train=tf.train.AdamOptimizer(0.1).minimize(loss)
prediction=tf.argmax(outputs,axis=2)
with tf.Session() as sess:
  sess.run(tf.global_variables_initializer())



추가---


